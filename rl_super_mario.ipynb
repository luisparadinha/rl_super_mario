{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee5598be",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a23f4c3",
   "metadata": {},
   "source": [
    "## 1.1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c41d7d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "# import the game\n",
    "import gym_super_mario_bros\n",
    "\n",
    "# import the joypad wrapper\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# import the simplified controls\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7909701e",
   "metadata": {},
   "source": [
    "## 1.2. Setting up our game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86964539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisparadinha/reinforcement_learning_course/super_mario/.venv/lib/python3.11/site-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/luisparadinha/reinforcement_learning_course/super_mario/.venv/lib/python3.11/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/luisparadinha/reinforcement_learning_course/super_mario/.venv/lib/python3.11/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "# Setup game environment\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT) # wrap the environment with simplified controls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78930a29",
   "metadata": {},
   "source": [
    "## 1.3. Testing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb42c7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisparadinha/reinforcement_learning_course/super_mario/.venv/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/luisparadinha/reinforcement_learning_course/super_mario/.venv/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n",
      "/home/luisparadinha/reinforcement_learning_course/super_mario/.venv/lib/python3.11/site-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
      "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n",
      "/home/luisparadinha/reinforcement_learning_course/super_mario/.venv/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:280: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/luisparadinha/reinforcement_learning_course/super_mario/.venv/lib/python3.11/site-packages/gym_super_mario_bros/smb_env.py:148: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m     state = reset_out[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(reset_out, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m reset_out \u001b[38;5;66;03m# gymnasium vs gym compatibility\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# take a random action\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m step_out = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43maction_space\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# env.step allows to take an action in the game, in this case randomnly through sample method - check cell below\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# unpack the outputs\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(step_out) == \u001b[32m5\u001b[39m:  \u001b[38;5;66;03m# gymnasium vs gym compatibility\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/reinforcement_learning_course/super_mario/.venv/lib/python3.11/site-packages/nes_py/wrappers/joypad_space.py:74\u001b[39m, in \u001b[36mJoypadSpace.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[33;03mTake a step using the given action.\u001b[39;00m\n\u001b[32m     61\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     71\u001b[39m \n\u001b[32m     72\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# take the step and record the output\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_action_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/reinforcement_learning_course/super_mario/.venv/lib/python3.11/site-packages/gym/wrappers/time_limit.py:60\u001b[39m, in \u001b[36mTimeLimit.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[32m     49\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[32m     50\u001b[39m \n\u001b[32m     51\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     57\u001b[39m \u001b[33;03m        \"TimeLimit.truncated\"=False if the environment terminated\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     59\u001b[39m     observation, reward, terminated, truncated, info = step_api_compatibility(\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     61\u001b[39m         \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     62\u001b[39m     )\n\u001b[32m     63\u001b[39m     \u001b[38;5;28mself\u001b[39m._elapsed_steps += \u001b[32m1\u001b[39m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._elapsed_steps >= \u001b[38;5;28mself\u001b[39m._max_episode_steps:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/reinforcement_learning_course/super_mario/.venv/lib/python3.11/site-packages/gym/wrappers/order_enforcing.py:37\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling env.reset()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/reinforcement_learning_course/super_mario/.venv/lib/python3.11/site-packages/gym/wrappers/step_api_compatibility.py:52\u001b[39m, in \u001b[36mStepAPICompatibility.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[32m     44\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment, returning 5 or 4 items depending on `new_step_api`.\u001b[39;00m\n\u001b[32m     45\u001b[39m \n\u001b[32m     46\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m \u001b[33;03m        (observation, reward, terminated, truncated, info) or (observation, reward, done, info)\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     step_returns = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.new_step_api:\n\u001b[32m     54\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m step_to_new_api(step_returns)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/reinforcement_learning_course/super_mario/.venv/lib/python3.11/site-packages/gym/wrappers/env_checker.py:39\u001b[39m, in \u001b[36mPassiveEnvChecker.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m.env, action)\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/reinforcement_learning_course/super_mario/.venv/lib/python3.11/site-packages/nes_py/nes_env.py:300\u001b[39m, in \u001b[36mNESEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    298\u001b[39m \u001b[38;5;28mself\u001b[39m.controllers[\u001b[32m0\u001b[39m][:] = action\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# pass the action to the emulator as an unsigned byte\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mStep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_env\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[38;5;66;03m# get the reward for this step\u001b[39;00m\n\u001b[32m    302\u001b[39m reward = \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m._get_reward())\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Create a flag - tells the game that we need to start the game on\n",
    "done = True\n",
    "# loop through each frame in the game\n",
    "for step in range(100000):\n",
    "    # start the game if the flag is true\n",
    "    if done:\n",
    "        # start the game and get the initial state\n",
    "        reset_out = env.reset()\n",
    "        state = reset_out[0] if isinstance(reset_out, tuple) else reset_out # gymnasium vs gym compatibility\n",
    "        \n",
    "    # take a random action\n",
    "    step_out = env.step(env.action_space.sample()) # env.step allows to take an action in the game, in this case randomnly through sample method - check cell below\n",
    "    \n",
    "    # unpack the outputs\n",
    "    if len(step_out) == 5:  # gymnasium vs gym compatibility\n",
    "        state, reward, done, truncated, info = step_out\n",
    "        done = done or truncated\n",
    "    else:\n",
    "        state, reward, done, info = step_out\n",
    "\n",
    "    # render the game\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8df0e29",
   "metadata": {},
   "source": [
    "## 1.4. Annex code for interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253e4341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['NOOP'],\n",
       " ['right'],\n",
       " ['right', 'A'],\n",
       " ['right', 'B'],\n",
       " ['right', 'A', 'B'],\n",
       " ['A'],\n",
       " ['left']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- IGNORE ---\n",
    "# Just to verify everything is set up correctly\n",
    "SIMPLE_MOVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2570cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['right']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- IGNORE ---\n",
    "# check a random action from the 7 available actions in\n",
    "SIMPLE_MOVEMENT[env.action_space.sample()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e010928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IGNORE ---\n",
    "# Just to verify everything is set up correctly\n",
    "print(f'The number of actions is: {env.action_space}') # check the number of actions after wrapping\n",
    "print(f'The observation space shape is: {env.observation_space.shape}') # check the observation space shape after wrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e43c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 256, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- IGNORE ---\n",
    "# Check the shape of the observation space\n",
    "state = env.reset()\n",
    "state.shape # here we can see the shape of the observation space - in this case a (240, 256, 3) array representing the RGB image of the game screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c576b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[104 136 252]\n",
      "  [104 136 252]\n",
      "  [104 136 252]\n",
      "  ...\n",
      "  [104 136 252]\n",
      "  [104 136 252]\n",
      "  [104 136 252]]\n",
      "\n",
      " [[104 136 252]\n",
      "  [104 136 252]\n",
      "  [104 136 252]\n",
      "  ...\n",
      "  [104 136 252]\n",
      "  [104 136 252]\n",
      "  [104 136 252]]\n",
      "\n",
      " [[104 136 252]\n",
      "  [104 136 252]\n",
      "  [104 136 252]\n",
      "  ...\n",
      "  [104 136 252]\n",
      "  [104 136 252]\n",
      "  [104 136 252]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[240 208 176]\n",
      "  [228  92  16]\n",
      "  [228  92  16]\n",
      "  ...\n",
      "  [228  92  16]\n",
      "  [228  92  16]\n",
      "  [  0   0   0]]\n",
      "\n",
      " [[240 208 176]\n",
      "  [228  92  16]\n",
      "  [228  92  16]\n",
      "  ...\n",
      "  [228  92  16]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]]\n",
      "\n",
      " [[228  92  16]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  ...\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [228  92  16]]]\n",
      "0.0\n",
      "False\n",
      "{'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 41, 'x_pos_screen': 41, 'y_pos': 79}\n"
     ]
    }
   ],
   "source": [
    "# --- IGNORE ---\n",
    "# Just to verify what's inside the outputs of env.step method \n",
    "len(env.step(1)) # check the number of outputs from env.step method\n",
    "# 4 outputs in gym: state, reward, done, info\n",
    "# 5 outputs in gymnasium: state, reward, done, truncated, info\n",
    "\n",
    "# first output is the state\n",
    "print(env.step(1)[0]) # state\n",
    "\n",
    "# second output is the reward\n",
    "print(env.step(1)[1]) # reward - in this case 0.0 because Mario got stuck in a pipe\n",
    "\n",
    "# third output is the done flag\n",
    "print(env.step(1)[2]) # done - in this case False because the game is not over (not dead)\n",
    "\n",
    "# fourth output is the info dictionary\n",
    "print(env.step(1)[3]) # info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5011f045",
   "metadata": {},
   "source": [
    "# 2. Preprocess Environment\n",
    "\n",
    "We need to preprocess our Mario game data before we run any algorithm on it. In this case we'll be applying two preprocessing steps:\n",
    "- Grayscaling: Colored images have tripple the data to process (RGB layers)\n",
    "- Framestacking: Helps our algorithm to have context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d184ff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Frame Stack wrapper and GrayScaling wrapper\n",
    "from gym.wrappers import GrayScaleObservation, FrameStack\n",
    "# Import Vectorization wrapper\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "# Import matplotlib for plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
